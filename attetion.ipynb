{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2609ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calcular_Atencao(Q,K,V):\n",
    "    # Multiplicação da consulta com a etiqueta transformada\n",
    "    K_transpose = np.transpose(K)\n",
    "    score =np.matmul(Q,K_transpose)\n",
    "    #Escalonamento\n",
    "    scale = np.sqrt(K.shape[-1])\n",
    "    terminado = score/scale\n",
    "    #Padronização\n",
    "    softmax = np.exp(terminado - np.max(terminado, axis=-1, keepdims=True))\n",
    "\n",
    "    pesos = softmax / np.sum(softmax, axis=-1, keepdims=True)\n",
    "\n",
    "    resultado = np.matmul(pesos,V)\n",
    "\n",
    "    return resultado, pesos\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[1, 2, 2],[1,1,2]], dtype=np.int8)\n",
    "K = np.array([[2,5,6],[0,8,1]], dtype=np.int8)\n",
    "V = np.array([[10,50],[20,60]], dtype=np.int8)\n",
    "\n",
    "Calcular_Atencao(Q,K,V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
